{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "studied-whale",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Basics of Clustering Methods for Linguistic Data\n",
    "\n",
    "## Society for Linguistics Undergrad Students (SLUgS) March 31, 2022 - Eric Wilbanks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "white-albert",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 1) Overview\n",
    "\n",
    "- who is this for?\n",
    "    - folks interested in language data\n",
    "    - no experience with statistics or coding required (though basic Python knowledge will help you understand the examples)\n",
    "    \n",
    "- what are we doing?\n",
    "    - motivation of clustering methods\n",
    "    - basic types of clustering approaches and their uses\n",
    "    - example python code and hands-on activities\n",
    "    \n",
    "- what do you need to get started?\n",
    "    - review the README.md file\n",
    "    - consider using Datahub (or your own local Jupyter installation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aquatic-spencer",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# load dependencies\n",
    "from urllib import request # for downloading\n",
    "import pandas as pd # for data manipulations\n",
    "pd.options.mode.chained_assignment = None  # default='warn'; since we're only doing column additions in place, this is fine. Use at your own peril though...\n",
    "import numpy as np # for everything really\n",
    "import matplotlib as mpl # for plotting\n",
    "import seaborn as sns # for plotting\n",
    "from sklearn import cluster, mixture # clustering methods\n",
    "from scipy import linalg, stats\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-night",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2) Clustering Basics\n",
    "\n",
    "**Why use them? Why do we care about clusters?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "persistent-package",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- linguistic data are hierarchically structured, e.g.:\n",
    "    - individual vowel utterances are grouped within phonemes\n",
    "    - sentences are grouped under authors\n",
    "    - languages are grouped under language families"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-disney",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- we might go in with hypotheses about the structure of the system, or we might want to discover this structure.\n",
    "- in any case, clustering algorithms are one approach to quantifying this structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "municipal-wisdom",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- we'll mainly be looking at 2D and 1D clustering, but most methods scale to higher order problems as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-interview",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Clustering Methods are very diverse, but we can characterize many of them by how they go about finding clusters:\n",
    "- ***Centroids***: picking a central cluster point and assigning points to clusters depending on their distance to this centroid (e.g., k-means)\n",
    "- ***Distributions***: fitting probability distributions to categories and using those to get a measure of each data point's likely category assignment (e.g., Gaussian Mixture Models)\n",
    "- ***Neighborhood Densities***: like centroid-based methods, density algorithms look at the relationship between points, but take into account the density of adjacent neighbors. This allows for less influence of outliers and better identification of core clusters. (e.g., DBSCAN)\n",
    "- ***Hierarchical Structures***: define clusters on the basis of connection between data points (e.g., dendrograms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appropriate-boards",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When you're choosing a clustering algorithm for your task/data, you'll need to consider questions such as:\n",
    "\n",
    "- Will all the clusters have the same number/density of points?\n",
    "- Will they be the same shape? The same size?\n",
    "- How variable will they be? How many outliers are in the data?\n",
    "- How many data points will I have?\n",
    "- Can a data point be members of multiple clusters (fuzzy/soft clustering) or is it only ever in one cluster?\n",
    "\n",
    "We're going to start with some basic algorithms today, since deciding between the various options is beyond the scope of this workshop!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chronic-beauty",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3) K-Means Clustering : Vowel Measurements\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endangered-budapest",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# download the csv of vowel measurements\n",
    "spade_buckeye_url = 'https://osf.io/c5u7a/download'\n",
    "spade_buckeye_csv = './spade_buckeye.csv'\n",
    "request.urlretrieve(spade_buckeye_url, spade_buckeye_csv)\n",
    "# read in as pandas dataframe\n",
    "vowels = pd.read_csv(spade_buckeye_csv)\n",
    "# remove rows where we don't have F1 measurements\n",
    "vowels = vowels[~vowels['F1'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sporting-doctrine",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3.1 Look at the data!\n",
    "\n",
    "Your very first step should always be to take a look at the data. \n",
    "\n",
    "What are we dealing with? What are the variables we have? How many speakers are in this dataset? What do the vowel measurements look like? What are these non-IPA symbols? (hint: it's lower-case [Arpabet](https://en.wikipedia.org/wiki/ARPABET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-lemon",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# variables in our set\n",
    "vowels.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brief-accessory",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# get number of observations per-speaker\n",
    "print(vowels.groupby('speaker').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-compact",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# now per-phone\n",
    "print(vowels.groupby('phone_label').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technical-library",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Vowel formant measurements vary systematically by speakers, in part due to differences in vocal-tract length.\n",
    "\n",
    "Ideally, we want to compare between-speaker measurements after normalizing for these differences.\n",
    "\n",
    "There are a lot of options, but a useful first pass method is **z-score normalization**.\n",
    "- zscore transform all measurements of a given speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sexual-chain",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# grouping by speaker and then zscoring F2 and F1\n",
    "zscores = vowels.groupby('speaker')[['F2','F1']].transform(stats.zscore).rename(columns={'F2':'F2.z', 'F1':'F1.z'})\n",
    "# append new columns\n",
    "vowels = pd.concat([vowels, zscores], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-position",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# define our phone_labels for a subset of vowels\n",
    "subset_vowels = ['iy','aa','uw']\n",
    "# subset vowels to just be in the subset\n",
    "subset = vowels[vowels['phone_label'].isin(subset_vowels)]\n",
    "# plot F1 and F2: standard and z-score\n",
    "fig, (ax1,ax2) = mpl.pyplot.subplots(1,2)\n",
    "# individual scatterplots\n",
    "sns.scatterplot(x='F2', y='F1', data=subset, hue='phone_label', ax=ax1)\n",
    "sns.scatterplot(x='F2.z', y='F1.z', data=subset, hue='phone_label', ax=ax2)\n",
    "# titles\n",
    "ax1.set_title('Standard Hz')\n",
    "ax2.set_title('Z-Scored')\n",
    "# flip axes for typical orientation\n",
    "ax1.invert_xaxis() \n",
    "ax1.invert_yaxis()\n",
    "ax2.invert_xaxis()\n",
    "ax2.invert_yaxis()\n",
    "# spacing\n",
    "fig.tight_layout()\n",
    "# show plot\n",
    "mpl.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-mapping",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3.2 k-means algorithm basics\n",
    "\n",
    "k-means clustering is a broad family of related algorithms that are all ***centroid*** based methods. \n",
    "\n",
    "The basic process is:\n",
    "1. Randomly assign all data points to k number of clusters\n",
    "2. Calculate the mean/center of each cluster\n",
    "3. Reassign each data point to the nearest cluster center\n",
    "4. Repeat 2-3 until we converge on a stable assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-belief",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# pull out just the F2.z and F1.z columns\n",
    "formants = subset.loc[:,['F2.z','F1.z']]\n",
    "# specify number of clusters\n",
    "k_subset_zscore = cluster.KMeans(n_clusters = 3).fit(formants)\n",
    "# add identified cluster labels to dataframe\n",
    "subset['cluster_label'] = k_subset_zscore.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-falls",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# plot F2.z and F1.z against predicted cluster labels\n",
    "ax = sns.scatterplot(x='F2.z', y='F1.z', data=subset, hue='cluster_label')\n",
    "center_x, center_y = zip(*k_subset_zscore.cluster_centers_) # zip(*) to convert [[x1,y1],[x2,y2] to [(x1,x2),(y1,y2)]\n",
    "ax.scatter(center_x, center_y, marker='P', color='purple', s=100)\n",
    "ax.invert_xaxis() # flip axis for typical orientation\n",
    "ax.invert_yaxis()\n",
    "mpl.pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-major",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# visualize crosstab percentages\n",
    "pd.crosstab(subset['phone_label'], subset['cluster_label'], normalize='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-magic",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# plot F2.z and F1.z against manual and automatic clustering\n",
    "ax = sns.scatterplot(x='F2.z', y='F1.z', data=subset, hue=subset[['phone_label', 'cluster_label']].apply(tuple, axis=1))\n",
    "ax.scatter(center_x, center_y, marker='P', color='purple', s=100)\n",
    "ax.invert_xaxis() # flip axis for typical orientation\n",
    "ax.invert_yaxis()\n",
    "mpl.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solved-dispute",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3.3 Extension - Unnormalized\n",
    "\n",
    "Now, let's do the same analysis on the unnormalized formant measurements. How does the automatic clustering compare to the clustering on the z-scored measurements?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-prior",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# pull out just the F2 and F1 columns\n",
    "#formants_un = \n",
    "\n",
    "# specify number of clusters\n",
    "#k_subset_un = \n",
    "\n",
    "# add identified cluster labels to dataframe\n",
    "#subset['cluster_label_un'] = \n",
    "#center_x, center_y = zip(*k_subset_un.cluster_centers_) # zip(*) to convert [[x1,y1],[x2,y2] to [(x1,x2),(y1,y2)]\n",
    "\n",
    "# plot F2.z and F1.z against manual and automatic clustering\n",
    "#ax = sns.scatterplot(x='F2', y='F1', data=subset, hue=subset[[??]].apply(tuple, axis=1))\n",
    "#ax.scatter(center_x, center_y, marker='P', color='purple', s=100)\n",
    "#ax.invert_xaxis() # flip axis for typical orientation\n",
    "#ax.invert_yaxis()\n",
    "#mpl.pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compressed-adobe",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# visualize crosstab percentages\n",
    "#print(pd.crosstab(subset['phone_label'], subset['cluster_label_un'], normalize='index'))\n",
    "#print()\n",
    "# visualize normalized percentages to compare\n",
    "#print(pd.crosstab(subset['phone_label'], subset['cluster_label'], normalize='index'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conventional-proportion",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. How many clusters do I need?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "middle-evidence",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Depending on your question and domain, you might not want to/be able to specify the number of clusters a priori.\n",
    "\n",
    "One method of evaluating the appropriate number of clusters is the ***elbow method***."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-charles",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "More clusters will always explain more of the variance in the data, but at a certain point the increased variance explained for each cluster will become negligible. We want to find the \"elbow\" in the graph of number of clusters vs. variance explained, to optimize for predictiveness with the fewest number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afraid-uncertainty",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "sklearn.kmeans has a built-in implementation of \"inertia\" (within-category sum-of-squares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brazilian-grant",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "inertias = {} # container for inertia\n",
    "for k in range(1,7):\n",
    "    # specify number of clusters\n",
    "    k_subset_zscore = cluster.KMeans(n_clusters = k).fit(formants)\n",
    "    inertias[k] = k_subset_zscore.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "married-miracle",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "mpl.pyplot.plot(inertias.keys(), inertias.values())\n",
    "mpl.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-cycle",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In this case, the elbow-method points to k=2 to be the likely best number of clusters.\n",
    "\n",
    "However, with our expert linguistic knowledge we're sure that the \"correct\" number of clusters is 3. \n",
    "\n",
    "What's going on here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-server",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# specify number of clusters\n",
    "k_subset_z2 = cluster.KMeans(n_clusters = 2).fit(formants)\n",
    "# add identified cluster labels to dataframe\n",
    "subset['cluster_label_z2'] = k_subset_z2.labels_\n",
    "center_x, center_y = zip(*k_subset_z2.cluster_centers_) # zip(*) to convert [[x1,y1],[x2,y2] to [(x1,x2),(y1,y2)]\n",
    "# plot F2.z and F1.z against manual and automatic clustering\n",
    "ax = sns.scatterplot(x='F2.z', y='F1.z', data=subset, hue=subset[['phone_label', 'cluster_label_z2']].apply(tuple, axis=1))\n",
    "ax.scatter(center_x, center_y, marker='P', color='purple', s=100)\n",
    "ax.invert_xaxis() # flip axis for typical orientation\n",
    "ax.invert_yaxis()\n",
    "mpl.pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caroline-princeton",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pd.crosstab(subset['phone_label'], subset['cluster_label_z2'], normalize='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-copper",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4.1 Extension - tense vowels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-vegetable",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# define our phone_labels for the tense vowels\n",
    "tense_vowels = ['aa','ey','iy','ow','uw']\n",
    "# subset vowels to just be in the tense set\n",
    "tense = vowels[vowels['phone_label'].isin(tense_vowels)]\n",
    "# plot F1 and F2\n",
    "ax = sns.scatterplot(x='F2.z', y='F1.z', data=tense, hue='phone_label')\n",
    "ax.invert_xaxis() # flip axis for typical orientation\n",
    "ax.invert_yaxis()\n",
    "mpl.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respiratory-newsletter",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, using the `tense` dataset above and use the k-means clustering algorithm with k=5.\n",
    "\n",
    "Then, visualize the cluster assignments using a plot like we've seen before.\n",
    "\n",
    "What types of clusters do we end up with? Are they linguistically reasonable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-crest",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#tense_formants_z = ??\n",
    "#k_tense_z = ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developmental-beatles",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#tense['cluster_label'] = k_tense_z.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bearing-cross",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# plot F1 and F2\n",
    "#ax = sns.scatterplot(x='F2.z', y='F1.z', data=tense, hue='cluster_label')\n",
    "#center_x, center_y = zip(*k_tense_z.cluster_centers_)\n",
    "#ax.scatter(center_x, center_y, marker='P', color='red', s=100)\n",
    "#ax.invert_xaxis() # flip axis for typical orientation\n",
    "#ax.invert_yaxis()\n",
    "#mpl.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-madness",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 5. GMM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-scholarship",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's consider a different clustering method which focuses on maximizing probability and fitting distributions to the data: Gaussian Mixture Models (GMMs).\n",
    "\n",
    "This approach is useful especially when you have some reason to believe that your data-generating process might involve Gaussian (normal) distributions (such as if speakers have a shared acoustic target and (normally-distributed) noise is introduced during vowel production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "critical-telephone",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "tense_formants_z = tense.loc[:,['F2.z','F1.z']]\n",
    "# initialize a GMM with 5 components/clusters\n",
    "g_tense = mixture.GaussianMixture(n_components=5)\n",
    "# fit this GMM to the data\n",
    "g_tense.fit(tense_formants_z)\n",
    "# pull out the most likely category label and save as a df colun\n",
    "tense['gmm_label'] = g_tense.predict(tense_formants_z)\n",
    "tense = tense.reset_index(drop=True)\n",
    "\n",
    "# use our category models to predict the probability of each data point being classified as each cluster\n",
    "probs = pd.DataFrame(g_tense.predict_proba(tense_formants_z)).rename(columns={0:'prob_0',1:'prob_1',2:'prob_2',3:'prob_3',4:'prob_4'})\n",
    "tense = pd.concat([tense,probs], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-greenhouse",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# what does `probs` look like?\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-courage",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def plot_ellipses(ax,fit):\n",
    "    # adapted from http://scikit-learn.org/stable/auto_examples/mixture/plot_gmm.html\n",
    "    for i, el in enumerate(fit.means_):\n",
    "        # calculate ellipse parameters\n",
    "        v, w = linalg.eigh(fit.covariances_[i])\n",
    "        v = 2.0 * np.sqrt(2.0) * np.sqrt(v)\n",
    "        u = w[0] / linalg.norm(w[0])\n",
    "        \n",
    "        # Plot an ellipse to show the Gaussian component\n",
    "        angle = np.arctan(u[1] / u[0])\n",
    "        angle = 180.0 * angle / np.pi  # convert to degrees\n",
    "        p = mpl.patches.Ellipse(fit.means_[i], v[0], v[1], 180.0 + angle, color='black')\n",
    "        p.set_alpha(0.5)\n",
    "        ax.add_artist(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-value",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# plot F1 and F2\n",
    "ax = sns.scatterplot(x='F2.z', y='F1.z', data=tense, hue = 'gmm_label', palette = 'colorblind')\n",
    "plot_ellipses(ax,g_tense)\n",
    "ax.invert_xaxis() # flip axis for typical orientation\n",
    "ax.invert_yaxis()\n",
    "mpl.pyplot.show()\n",
    "\n",
    "# plot F1 and F2\n",
    "ax = sns.scatterplot(x='F2.z', y='F1.z', data=tense, hue = 'phone_label', palette = 'colorblind')\n",
    "plot_ellipses(ax,g_tense)\n",
    "ax.invert_xaxis() # flip axis for typical orientation\n",
    "ax.invert_yaxis()\n",
    "mpl.pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-cancellation",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pd.crosstab(tense['phone_label'], tense['gmm_label'], normalize='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-cooling",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What's going on here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immune-burns",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We have a lot of data with a ton of variance. Some of that token-level variance is likely due to difference in phonological context, perhaps lexical factors, local speaking rate, individual speakers differing in linguistic systems, etc. \n",
    "\n",
    "This makes the clustering task extremely difficult, if the clustering methods are trying to accurately predict ALL the observed data.\n",
    "\n",
    "Let's consider what happens when we just look at speaker-means, averaging over some of these other influential factors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chinese-society",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# calculate by-speaker means\n",
    "tense_means = tense.groupby(['speaker','phone_label'])[['F2.z','F1.z']].mean().reset_index()\n",
    "tense_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insured-patio",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# fit a 5 cluster GMM to the tense_means df\n",
    "tense_formants_mean = tense_means.loc[:,['F2.z','F1.z']]\n",
    "g_tense_means = mixture.GaussianMixture(n_components=5)\n",
    "g_tense_means.fit(tense_formants_mean)\n",
    "tense_means['gmm_label'] = g_tense_means.predict(tense_formants_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-scoop",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# plot F1 and F2\n",
    "ax = sns.scatterplot(x='F2.z', y='F1.z', data=tense_means, hue = 'gmm_label', palette = 'colorblind')\n",
    "plot_ellipses(ax,g_tense_means)\n",
    "ax.invert_xaxis() # flip axis for typical orientation\n",
    "ax.invert_yaxis()\n",
    "mpl.pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strange-meter",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# compare ground-truth linguist classifications with cluster assignments \n",
    "# (note the results for k-means on a speaker-mean df are basically the same)\n",
    "pd.crosstab(tense_means['phone_label'], tense_means['gmm_label'], normalize='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuclear-frontier",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "aics = {} # container for AICs\n",
    "for k in range(1,10):\n",
    "    # specify number of clusters\n",
    "    g = mixture.GaussianMixture(n_components=k)\n",
    "    g.fit(tense_formants_z)\n",
    "    aics[k] = g.aic(tense_formants_z)\n",
    "\n",
    "mpl.pyplot.plot(aics.keys(), aics.values())\n",
    "mpl.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "julian-compiler",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 5.1 Extension - GMM for tense_means with 2 clusters\n",
    "\n",
    "You might be able to guess what the two likely clusters as noted by the elbow method would be. Briefly in this extension, carry out the analysis and visualize the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-semiconductor",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# 2 cluster GMM\n",
    "#g_tense_means2 = ??\n",
    "#g_tense_means2.fit(tense_formants_mean)\n",
    "#tense_means['gmm_label_2'] = ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-macedonia",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# plot F1 and F2\n",
    "#ax = sns.scatterplot(??)\n",
    "#plot_ellipses(ax,g_tense_means2)\n",
    "#ax.invert_xaxis() # flip axis for typical orientation\n",
    "#ax.invert_yaxis()\n",
    "#mpl.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "institutional-gallery",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 6. DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removed-polymer",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "DBSCAN is a density based clustering method that attempts to locate core regions and ignore outliers by considering each point's neighbors and how dense its local region is. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-specification",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Unlike the methods we've seen so far, DBSCAN automatically choose the optimal number of components/clusters based on the parameters we set and the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "important-celebrity",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# initialize model object\n",
    "tense_means_db = cluster.DBSCAN()\n",
    "# fit to data (just the F2 and F1 tense formants\n",
    "tense_means_db.fit(tense_formants_mean)\n",
    "# save cluster labels\n",
    "tense_means['db_labels'] = tense_means_db.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approximate-editing",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# plot F1 and F2\n",
    "ax = sns.scatterplot(x='F2.z', y='F1.z', data=tense_means, hue = 'db_labels', palette = 'colorblind')\n",
    "ax.invert_xaxis() # flip axis for typical orientation\n",
    "ax.invert_yaxis()\n",
    "mpl.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-supervision",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The most important parameter of the DBSCAN method is `eps`: short for epsilon, which here represents the maximum distance between two samples for them to be considered a neighbor.\n",
    "\n",
    "The default is 0.5. What happens when we set it to 0.2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-fever",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# initialize model object\n",
    "#tense_means_db_eps02 = ??\n",
    "# fit to data (just the F2 and F1 tense formants\n",
    "#??\n",
    "# save cluster labels\n",
    "#tense_means['db_labels_eps02'] = ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corresponding-overview",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# plot F1 and F2\n",
    "#ax = sns.scatterplot(x=??, y=??, data=tense_means, hue = ??, palette = 'colorblind')\n",
    "#ax.invert_xaxis() # flip axis for typical orientation\n",
    "#ax.invert_yaxis()\n",
    "#mpl.pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
